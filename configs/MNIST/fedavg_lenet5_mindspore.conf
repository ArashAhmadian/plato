[clients]

# The total number of clients
total_clients = 1

# The number of clients selected in each round
per_round = 1

# Should the clients compute test accuracy locally?
do_test = true

[data]

# Number of samples in each partition
partition_size = 60000

# IID, biased, or sharded?
divider = iid_mindspore

# Uniform or normal distribution for labels across clients?
label_distribution = uniform

[trainer]

type = basic_mindspore

# The maximum number of training rounds
rounds = 100

# Whether the training should use multiple GPUs if available
parallelized = false

# The target accuracy
target_accuracy = 0.98

# Number of epoches for local training in each communication round
epochs = 1

batch_size = 32

dataset = MNIST_mindspore
data_path = ./data

use_mindspore = true
model = lenet5_mindspore

optimizer = SGD
learning_rate = 0.01
momentum = 0.9

[server]

# Aggregation algorithm
type = fedavg

address = localhost
port = 8000

